---
layout:     post
title:      统计学习方法(1)---FM
subtitle:   FM，FFM，AFM，Bilinear-FFM
date:       2020-02-01
author:     weber
header-img: img/post-bg-ios9-web.jpg
catalog: true
mathjax: true
tags:
    - machine learning
    - 统计学习方法
---

# 1. 论文

- FM： [Factorization Machines](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf)，2010
- FFM：[Field-aware Factorization Machines for CTR Prediction](https://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf)，2016，RecSys
- AFM：[Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks](https://arxiv.org/pdf/1708.04617.pdf)，2017，IJCAI

# 2. 学习资料

- [gongyouliu - 因子分解机](https://mp.weixin.qq.com/s/SlflfdqSIZGR59ch50DIAA)
- [标点符 - CTR预估模型 FM，FFM，DeepFM](https://www.biaodianfu.com/ctr-fm-ffm-deepfm.html)
- [张俊林 - FFM及DeepFFM模型在推荐系统的探索](https://zhuanlan.zhihu.com/p/67795161)
- [文文 - FFM模型理论和实践](https://www.jianshu.com/p/781cde3d5f3d)
- [文文 - AFM模型理论和实践](https://www.jianshu.com/p/83d3b2a1e55d)

# 3. 简单总结

## 3.1 FM

FM（Factorization Machines）是在2010年提出的一种学习二阶特征交叉的模型。在原先线性模型的基础上，枚举了所有二阶交叉特征融入模型。

$$
f(x) = w_0 + \sum_{i=1}^nw_ix_i+\sum_{i=1}^n\sum_{j=i+1}^n<v_i,v_j>x_ix_j
$$

优点：
1. 有效处理稀疏场景下的特征学习；
2. 通过推导，具有线性的时间复杂度 O(kn)；
3. 能够对训练集中没出现的特征组合进行泛化；

缺点：
1. 特征组合仅考虑了二阶交叉；
2. 存在梯度耦合问题，一定程度上损失了模型的表达能力；

## 3.2 FFM

2016年，FFM（filed-aware factorization machines）提出了利用特征所属的场的信息来缓解梯度耦合方法，每个特征与不同场下的特征交互时都使用了不同的隐向量。

$$
f(x) = w_0 + \sum_{i=1}^nw_ix_i+\sum_{i=1}^n\sum_{j=i+1}^n<v_{i,f_j},v_{j,f_i}>x_ix_j
$$

优点：

1. 缓解梯度耦合，提升了模型的表达能力；
2. 效果好于FM；

缺点：

1. 参数量大，计算慢；

## 3.3 AFM

FM枚举了所有二阶交叉特征，但实际上有些交叉特征与我们的预估目标关联不大，AFM（attentional factorization machines）使用attention机制来学习不同二阶特征的重要性（这个思路与FFM一致，引入额外信息来表达不同特征交叉的重要性）

$$
f(x) = w_0 + \sum_{i=1}^nw_ix_i+p^T\sum_{i=1}^n\sum_{j=i+1}^n\alpha_{ij}(v_i\odot v_j)x_ix_j
$$

其中,$\alpha_{ij}=\frac{\exp(e_{ij})}{\sum_{i,j}\exp(e_{ij})}$，$e_{ij}=h^TReLU(W(v_i \odot v_j )x_ix_j + b)$

![image-20200323092237492](https://tva1.sinaimg.cn/large/00831rSTgy1gd3lngwi3mj31e60kate7.jpg)

优点：
1. 引入attention机制赋予不同交叉特征不同的重要程度，增加了一定可解释性。

缺点：
1. 仍然是浅层模型，没有学习到高阶的特征。

## 3.4 Bilinear-FFM

FFM效果好，但是参数量太大了。2018年，张俊林提出双线性FFM，降低参数的同时，保证效果。

$$
f(x) = w_0 + \sum_{i=1}^nw_ix_i+\sum_{i=1}^n\sum_{j=i+1}^n(v_iW_{(f_i,f_j)}v_j)x_ix_j
$$

![image-20200409141806889](https://tva1.sinaimg.cn/large/00831rSTly1gdnhq1u7p6j316g0j8tow.jpg)

优点：
1. 减少了FFM的参数的同时达到FFM的效果；

缺点：
1. 没有学习到高阶特征；

## 3. 5 效果对比

<img src="https://tva1.sinaimg.cn/large/00831rSTgy1gdnhy39c4aj319a0u0tf1.jpg" alt="image-20200409142539917" style="zoom: 33%;" />

# 4. 面试问题

- 4.1 FM，FFM，AFM的表达式，梯度，计算复杂度分析，参数量分析
- FM 计算复杂度线性优化的推导
- FM 优缺点
- FM vs MF：区别/联系

> 当只有 u 和 i 的 id 信息时，将 id 平铺，就有每个交互的输入特征 x，每个样本中有且只有两个特征不为 0 。
> ![img](https://tva1.sinaimg.cn/large/00831rSTly1gdnne2emlqj30qs03mglk.jpg)
>
> 其表达式为： $ y = w_o + w_u + w_i + <v_u,v_i>$，就是含有bias 的矩阵分解的表达式。

- FM vs SVM：区别/联系/优缺点

> 多项式核函数为：$K(x,z) = (<x,z>+1)^d$
>
> 二姐多项式SVM方程为： $y = w_0 + \sqrt2 \sum_{i=1}^nw_ix_i + \sum_{i=1}^nw_{i,i}^{(2)}x_i^2 + \sqrt 2 \sum_{i=1}^{i-1}\sum_{j=i+1}^n w_{ij}^{(2)}x_ix_j$
>
> 从表达式来看，区别有二：
>
> SVM多了自身的二阶交叉项；SVM的二阶交叉项的系数是独立的。

- FM 如何基于FTRL在线训练
- 考察知识深度，是否了解FM与深度学习结合的模型和方法：如DeepFM，FNN，NFM，DCN，xDeepFM，NFFM(ONN)，DeepFFM，FLEN等，各自的设计思路和优缺点。


