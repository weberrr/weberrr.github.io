---
layout:     post
title:      统计学习方法---EM
subtitle:   EM，GMM
date:       2020-02-05
author:     weber
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - machine learning
---

Expectation  Maximization，EM

# 1. 一句话介绍EM

EM是一种用于估计含有隐变量的概率模型的参数的迭代算法。

关键词：含有隐变量的模型，参数估计，迭代算法

## 1.1 EM步骤

输入：观察数据 $\{y_1.y_2,...,y_n\}$，联合分布 $P(y,z｜\theta)$，条件分布$P(z｜y,\theta)$，迭代次数$J$

输出：模型参数 $ \theta $

过程：

1. 随机初始化参数，记为$\theta^{(0)}$

2. E步，利用当前估计的参数值，求出在该参数下隐含变量的条件概率期望：

$$
Q_i(z_i) = P(z_i|y_i,\theta^{(j)})
$$

$$
Q(\theta,\theta^{(j)}) = \sum_{i=1}^n\sum_{z_i} Q_i(z_i)\log (P(y_i,z_i|\theta)
$$

3. M步，计算使得Q函数最大的参数值：	

$$
\theta^{(j+1)} = arg \max_{\theta} Q(\theta,\theta^{(j)})
$$

4. 重复2，3至收敛。

## 1.2 EM推导

对于给定的观察数据 $\{y_1.y_2,...,y_n\}$，我们的目标是找到参数 $\theta$，极大化模型分布的对数似然函数：

$$
\theta = \arg \max \sum_{i=1}^n \log P(y_i|\theta)
$$

如果数据含有隐变量$\{z_1,z_2,...,z_m\}$，则有：

$$
\theta = \arg\max \sum_{i=1}^n \log\sum_{z_i} P(y_i,z_i|\theta)
$$

上式无法直接求，所以使用Jensen不等式缩放：

$$
\begin{aligned}
\sum_{i=1}^n \log\sum_{z_i} P(y_i,z_i|\theta)
&= \sum_{i=1}^n \log\sum_{z_i} Q(z_i) \frac{P(y_i,z_i|\theta)}{Q(z_i)} \\
& \geq \sum_{i=1}^n \sum_{z_i} Q(z_i) \log \frac{P(y_i,z_i|\theta)}{Q(z_i)}
\end{aligned}
$$

其中 $\sum_{z_i} Q(z_i) = 1$，要取得等号，需要$\frac{P(y_i,z_i|\theta)}{Q(z_i)} = c$  

由上面两个式子，解得：

$$
Q(z_i) = \frac{P(y_i,z_i|\theta)}{\sum_z P(y_i,z_i|\theta)} = \frac{P(y_i,z_i|\theta)}{P(y_i|\theta)} = P(z_i|y_i,\theta)
$$

回到之前的式子，我们的最大化目标为：

$$
\arg\max_\theta \sum_{i=1}^n \sum_{z_i} Q(z_i) \log \frac{P(y_i,z_i|\theta)}{Q(z_i)}
$$

可以去掉第二项分母上的 $Q(z_i)$，它是一个常数项，得：

$$
\arg\max_\theta \sum_{i=1}^n \sum_{z_i} Q(z_i) \log P(y_i,z_i|\theta)
$$

这就是我们的Q函数。

# 2. EM扩展-GMM

## 2.1 高斯混合模型

高斯混合模型（Gaussian Mixture Model，GMM）是多个高斯分布的模型混合而成，其表达式为：

$$
p(y|\theta) = \sum_{k=1}^K \alpha_k \phi(y|\theta_k)
$$

其中，$\phi(y｜\theta_k) = \frac{1}{\sqrt{2\pi }\sigma_k} \exp(-\frac{(y-\mu_k)^2}{2\sigma_k^2})$

## 2.2 EM估计参数

确定隐变量 $\gamma_{jk} \in \mathbb{R}^{N \times K}$：

$$
\gamma _{jk} = \left \{
\begin{array}{}
1, 如果第j个观测来自第k个分模型\\
0, not
\end{array}
\right.
$$

确定联合分布：

$$
\begin{aligned}
P(\gamma_{jk}=1,y_j|\theta_k) &= P(y_j|\theta_k)\\
 &= \alpha_{k}\phi(y_j|\theta_k)
\end{aligned}
$$

确定条件分布：

$$
\begin{aligned}
\hat \gamma_{jk} & =  P(\gamma_{jk}=1|y_j,\theta_k)\\
& = \frac{P(\gamma_{jk}=1,y_j|\theta_k)}{\sum_kP(\gamma_{jk}=1,y_j|\theta_k)} \\
& = \frac{\alpha_{k}\phi(y_j|\theta_k)}{\sum_k \alpha_{k}\phi(y_j|\theta_k)}
\end{aligned}
$$

E步，求出Q函数：

$$
\begin{aligned}
Q(\theta,\theta^{(i)}) &= \sum_{j=1}^N\sum_{k=1}^K P(\gamma_{jk}=1|y_j,\theta_k)  \log P(\gamma_{jk}=1,y_j|\theta_k)  \\
&= \sum_{k=1}^K(n_k \log \alpha_k + \hat\gamma_{jk} [\log\frac{1}{\sqrt {2\pi}}-\log \sigma_k - \frac{1}{2\sigma^2_k}(y_j-\mu_k)^2])
\end{aligned}
$$

其中，$n_k = \sum_{i=1}^N \hat \gamma_{jk}$

M步，求极大值：对 $\mu_k$ ，$\sigma_k$，求导即可得到，而$\alpha_k$可以利用和为1的条件下求偏导数得到。

# 3. EM用于聚类

EM可以将样本的潜在类别看作是隐变量，将样本看作观察值，从而将聚类问题转化为参数估计问题。

以sklearn的包中的GMM来做聚类应用为例：

```python
from sklearn.mixture import GaussianMixture

gmm = GaussianMixture(n_components=1, covariance_type='full', max_iter=100)
```

- n_components：表示高斯混合模型的个数（聚类个数）。
- covariance_type：协方差类型，协方差的类型代表了不同的高斯混合模型的特征。

- max_iter：代表最大迭代次数。

# 4. EM面试问题总结

> EM是否收敛到全局最优？

EM 算法具备收敛性，但并不保证找到全局最大值，有可能找到局部最大值。解决方法是初始化几次不同的参数进行迭代，取结果最好的那次。


# 参考资料

[EM算法原理总结-刘建平](https://www.cnblogs.com/pinard/p/6912636.html)

[机器学习——经典十大算法之EM算法](https://mp.weixin.qq.com/s/724kLf67_0o8fN19Opuupg)

[【白话机器学习】算法理论+实战之EM聚类](https://mp.weixin.qq.com/s/n_n9mEHR15qLjbB7rLhmpA)