---
layout:     post
title:      推荐系统的多目标优化(1)-ESMM
subtitle:   Entire Space Multi-Task Model
date:       2020-05-01
author:     weber
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - 多目标
    - 论文笔记
    - recommender systems
---

论文：[Entire Space Multi-Task Model: An Effective Approach for Estimating Post-Click Conversion Rate](https://arxiv.org/pdf/1804.07931.pdf)，SIGIR，2018，阿里

目录：
[toc]
# 1. 提出背景

电商为例，用户的购买行为一般遵循顺序决策模式：impression -> click -> conversion。第一步的比例即CTR，第二步的比例即CVR。

>**impression**：用户观察到的曝光产品
>**click**：用户点击行为
>**conversion**：用户点击后的转化行为（如，购买，下载等）

![image-20200426162039694](https://tva1.sinaimg.cn/large/007S8ZIlly1ge78sw4u2nj317k0iatdx.jpg)

传统的CVR模型是采用点击->转化的样本进行预估，存在的问题：

1. **样本选择偏差(sample selection bias, SSB)**：如图，训练时使用点击样本空间训练，预测时使用整个样本空间推断。这样构建的训练样本与真实分布不一致，违背独立同分布假设，会影响模型的泛化性能。
2. **数据稀疏(data sparsity,DS)**：点击样本往往只占整体样本的x%，训练CVR模型的数据也就是训练CTR模型的数据的x%。用这么稀疏的数据训练，模型很难拟合。

阿里妈妈的算法同学借鉴多任务学习的思路，对用户的顺序式行为建模，提出ESMM。

# 2. ESMM模型

![image-20200426163015167](https://tva1.sinaimg.cn/large/007S8ZIlly1ge792rw7dxj31260u0qae.jpg)

全空间多任务模型（Entire Space Muti-Task Model，ESMM）结构如图，横向分为三层：Embedding层，Field-wise Pooling层，MLP层。纵向分为两个子任务：CVR和CTCVR。

## 2.1 Embedding层

借鉴DUPN和迁移学习的思想，ESMM把两个子任务的embedding层向量共享。文章中embedding维度为18。

>embedding层参数占整个网络参数的绝大部分。由于CTR任务的训练样本量要大大超过CVR任务的训练样本量，通过共享机制使得CVR子任务也能够从只有展现没有点击的样本中学习，从而缓解训练数据稀疏性问题。

## 2.2 Field-wise Pooling层

ESMM的文章很短只有4页，对这层ESMM里一字未提，去看了其实现源码，是用阿里的xdl实现的，需要读好久框架，找到了一份权威较高的阿里工程师基于tf.estimator的实现：[构建分布式Tensorflow模型系列:CVR预估之ESMM](https://zhuanlan.zhihu.com/p/42214716)，贴出代码该层一目了然：

```python
from tensorflow import feature_column as fc
# user field
pids = fc.categorical_column_with_hash_bucket("behaviorPids", 50000, dtype=tf.int64)
# item field
pid = fc.categorical_column_with_hash_bucket("productId", 50000, dtype=tf.int64)

pid_embed = fc.shared_embedding_columns([pids, pid], 100, combiner='sum',
                                        shared_embedding_collection_name="pid")
```

这里的pooling是对不定长的用户历史行为序列这种不定长的特征进行pooling，变为定长的embedding。去看了数据描述，这里对应的不定长用户行为特征有：商品类目ID，商品店铺ID，商品品牌ID，用户意图ID。

## 2.3 MLP层

MLP即正常的三层NN结构。看具体实现中，网络隐层为\[360,200,80,2\] (360为concat的emb总长度)，激活函数用的`prelu` (最后一层没激活函数)，最后优化器用的`adam`($\beta_1=0.999,\beta_2=0.9,\epsilon=10^{-8}$)。源码中未使用dropout，看到其他版本的开源中也会添加dropout用于降低过拟合。

![image-20200426174915526](https://tva1.sinaimg.cn/large/007S8ZIlly1ge7bieftl9j317408cdgr.jpg)

结构图中，所得到的 pCVR（粉色节点）仅是网络中的一个variable，没有显式的监督信号。而 pCTR 和 pCTCVR 都有对应的标签值。

## 2.4 损失函数

ESMM亮点之一就是通过全部样本（全空间）显式学习ctr,ctcvr，从而隐式学习cvr。其损失由两部分组成：
$$
L(\theta_{cvr},\theta_{ctr})=\sum_{i=1}^Nl(y_i,f(x_i;\theta_{ctr})) + \sum_{i=1}^Nl(y_i\&z_i,f(x_i;\theta_{ctr}) \times f(x_i;\theta_{cvr}))
$$

其中，l(·)为交叉熵损失函数。

# 3. 实验结果

## 3.1 实验数据

作者从淘宝日志中抽取整理了一个数据集Product，并开源了从Product中随机抽样1%构造的数据集 [Public](https://tianchi.aliyun.com/dataset/dataDetail?dataId=408&userId=1)（约38G），实验中，train:test=1:1，所有实验重复10次。

![image-20200426175748306](https://tva1.sinaimg.cn/large/007S8ZIlly1ge7blv054bj31hq0f8acc.jpg)

## 3.2 实验结果

**衡量指标** ：在点击样本上，计算CVR任务的AUC；同时前4个对比方法单独训练一个和BASE一样结构的CTR模型，均以pCTR*pCVR计算pCTCVR，在全部样本上计算CTCVR任务的AUC。

![image-20200426180138153](https://tva1.sinaimg.cn/large/007S8ZIlly1ge7bpuxo4qj31fy0hgwhm.jpg)

- BASE——图左部所示的CVR结构，训练集为点击集；

- AMAN——从unclicked样本中随机抽样作为负例加入点击集合（采样比 10%, 20%, 50%, 100%）；

- OVERSAMPLING——对点击集中的正例（转化样本）过采样（采样比 2,3,5,10 ）；

- UNBIAS——使用rejection sampling；

- DIVISION——分别训练CTR和CTCVR，相除得到pCVR；

- ESMM-NS——ESMM结构中CVR与CTR部分不share embedding。

![image-20200426183201524](https://tva1.sinaimg.cn/large/007S8ZIlly1ge7cm5tzbdj31am0l8gs6.jpg)

在Product数据集上做了训练集抽取率实验。ESMM有稳定的提升，同时也说明数据稀疏对结果影响大。

# 4. 思考

1. ESMM 根据用户 impression -> click -> conversion 的顺序决策模式，显示引入CTR和CTCVR作为辅助任务来隐式学习CVR，从而在完整样本空间下进行模型的训练和预测，解决了CVR预估中的2个难题。
2. ESMM 中的Base model很好替换，可以非常容易地和其他学习模型集成，从而吸收其他学习模型的优势，进一步提升学习效果，潜力空间巨大。
3. 文中没有提及正负样本比例的问题。但如此不平衡的数据应该需要对负样本进行降采样。在[蘑菇街的实践](https://zhuanlan.zhihu.com/p/76413089)中采用了正负样本1:6的采样比例。
4. 实践时，两部分损失可以考虑加权，也可以在最后排序的时候对两个打分进行加权，后者只需要训练一个模型，在serving的时候再更改权重，更为合理。
5. 在[蘑菇街的实践](https://zhuanlan.zhihu.com/p/76413089)中，ESMM存在长尾商品预估偏低的问题：低cvr的item预估偏高，高cvr的item预估偏低，分数差异小，过于集中。作者是通过根据数据分布进行校准来解决的。如何校准？



