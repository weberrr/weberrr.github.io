---
layout:     post
title:      召回策略演进梳理
subtitle:   召回策略
date:       2020-03-01
author:     weber
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
	- summary
---

实际的工业推荐系统，一般分为两个阶段。

- **召回**：主要根据用户部分特征，从海量的物品库里，快速找回一小部分用户潜在感兴趣的物品，然后交给排序环节；

- **精排**：排序环节可以融入较多特征，使用复杂模型，来精准地做个性化推荐。

召回强调快，排序强调准。本文主要对召回策略进行梳理，下篇会对排序策略进行梳理。

![img](https://tva1.sinaimg.cn/large/00831rSTgy1gdbr2pfj9jj31400lf40j.jpg)

# 1. 思维导图

标准的召回结构一般是多路召回，即将不同方法得到的召回结果整合起来喂给排序层。

召回的通用模式是离线训练Embedding，在线直接快速存取Embedding进行计算和召回。不同召回模型的区别在于使用什么样的模型进行离线训练。

![img](https://tva1.sinaimg.cn/large/00831rSTgy1gdbrpo8ss9j31400l3402.jpg)

我画出了召回的策略趋势发展图，基于策略和协同过滤的方法较为简单不再赘述，其余的召回方法会依照这个顺序进行介绍。

![image-20200330104125879](https://tva1.sinaimg.cn/large/00831rSTgy1gdbr9i9blkj31500u0n1k.jpg)

# 2. 基于Embedding

## 2.1 单Embedding召回

单embedding向量：每个user和item在一个时刻只用一个user embedding和item embedding向量去表示。

单embedding召回的主要思想：将user和item通过DNN映射到同一个低维度向量空间中，然后通过高效的检索方法去做召回。

### Youtube DNN召回（2016）

使用特征：用户观看过视频的embedding向量、用户搜索词的embedding向量、用户画像特征、context上下文特征等。

训练方式：三层ReLU神经网络之后接softmax层，去预测用户下一个感兴趣的视频，输出是在所有候选视频集合上的概率分布。训练完成之后，最后一层Relu的输出作为user embedding，softmax的权重可当做当前预测item的embedding表示。

线上预测：通过userId找到相应的user embedding，然后使用KNN方法找到相似度最高的top-N条候选结果返回。

![image-20200330104917873](https://tva1.sinaimg.cn/large/00831rSTgy1gdbrhp1pvdj30xv0u0nf1.jpg)

### Youtube 双塔模型召回（2019）

双塔模型的主要思想：两侧分别对user和item特征通过DNN输出向量，并在最后一层计算二个输出向量的内积。

![img](https://tva1.sinaimg.cn/large/00831rSTgy1gdbs2f8u75j30u00fbjsd.jpg)



用户侧特征：用户侧特征除了用户的基础特征外，主要是基于用户的历史观看记录来捕获用户的兴趣。比如使用用户最近观看过的k个视频的embedding的平均值。

视频侧特征：视频侧包含的特征既有类别特征如视频ID、频道ID，也有连续特征。类别特征中有分为单值类别特征和多值类别特征，对于多值类别特征，采用对embedding加权平均的方式得到最终的embedding。

训练Label：这里训练集Label并不是点击即为1，未点击为0。而是当一个视频被点击但是观看时长非常短时，label同样是0。当视频被完整看完时，label才是1。

实时更新：模型基于Tensorflow实现，并且进行了分布式实现。同时，模型会进行天级别更新。

## 2.2 多Embedding召回

### 阿里 Multi-Interest Network with Dynamic Routing（2019）

提出背景：电商场景下用户行为序列中的兴趣分布是多样的，因此如果只用一个 embedding 向量来表示用户的兴趣其表征能力是远远不够的。所以需要通过一种模型来建模出用户多个 embedding 的表示。

![image-20200330111744006](https://tva1.sinaimg.cn/large/00831rSTgy1gdbsbai0e8j31ti0u04nj.jpg)

离线训练：Multi-Interest Extractor Lyaer 负责建模用户多个兴趣向量 embedding，然后通过 Label-aware Attention 结构赋予多个兴趣向量不同的注意力权值，每次计算该用户的每个兴趣向量 embedding 与推荐 item的。

线上服务：在线计算用户的多个兴趣向量后，每个兴趣向量 embedding 通过 KNN 检索得到最相似的 Top-N 候选商品集合。

优势：多兴趣点召回，避免了单Embedding召回头部的问题。

> 召回阶段有时候容易碰到头部问题，就是比如通过用户兴趣embedding拉回来的物料，可能集中在头部优势领域中，造成弱势兴趣不太能体现出来的问题。如果把用户兴趣进行拆分，每个兴趣embedding各自拉回部分相关的物料，则可以很大程度缓解召回的头部问题。

## 2.3 GraphEmbedding

图神经网络的最终目的是要通过一定技术手段，获得图中节点的embedding编码。

通过CNN等信息聚合器，在图节点上进行计算，并反复迭代更新图节点的embedding，就能够最终获得可靠的图节点embedding信息，而这种迭代过程，其实体现的是远距离的节点将信息逐步通过图结构传递信息的过程，所以图结构是可以进行知识传递和补充的。

### Pinterest PinSage（2018）

提出背景：用户在Pinterest中，自己感兴趣的东西用图钉(pin)钉在钉板(broad)，包扩10亿pin ，10亿board 以及180亿边（若pin在broad中，则它们之间存在一条边）。因此可以利用 pin-broard 二分图结构和属性（如描述文本、图片特征），生成pin的高质量的embedding，用于推荐任务（如相关pin的推荐）。

![image-20200330113631176](https://tva1.sinaimg.cn/large/00831rSTly1gdbsuxztwsj31kc0k8wnc.jpg)

训练过程：对于一个 node 需要聚合 K 次，每次都通过聚合函数 aggregator 将上一层中与当前 node 有邻接关系的多个 nodes 与当前 node 拼接后，聚合一次，如此反复聚合 K 次，得到该 node 最后的特征。最下面一层的 node 特征就是输入的 node features。

![img](https://tva1.sinaimg.cn/large/00831rSTly1gdbsz59c70j30rq0bqabq.jpg)

优势：比较适合解决冷启动问题（内容信息+结构传递）

> 图的一个优势是，比较便于把协同信息、用户行为信息、内容属性信息等各种异质信息在一个统一的框架里进行融合，并统一表征为embedding的形式，这是它独有的一个优势，做起来比较自然。
>
> 另外的一个特有优势，就是信息在图中的传播性，所以对于推荐的冷启动以及数据稀疏场景应该特别有用。

# 3. 基于KG

物品之间是有一些知识联系存在的，即知识图谱，而这类数据是可以考虑用来增强推荐效果的，尤其是对于用户行为数据稀疏的场景，或者冷启动场景。举例来说，电影“泰坦尼克号”的主演是莱昂纳多，于是可以推荐其它由莱昂纳多主演的电影给这个用户。

知识图谱是一种信息拓展的模式，很明显，对知识进行近距离的拓展，这可能会带来信息补充作用，但是如果拓展的比较远，或者拓展不当，反而可能会引入噪音。

### 美团 RippleNet（2018）

![image-20200330115254066](https://tva1.sinaimg.cn/large/00831rSTgy1gdbtc9m4wgj31ow0pskg0.jpg)

做法：RippleNet通过将用户的历史行为作为seed，在KG中进行偏好传播，获取含有知识扩充的用户的embedding表示。

优势：缓解冷启动，发现用户的潜在偏好。

# 4. 基于TDM

深度树匹配（Tree-based Deep Match，TDM）是为大规模推荐系统设计的、能够承载任意先进模型 ( 也就是可以通过任何深度学习推荐模型来训练树 ) 来高效检索用户兴趣的推荐算法解决方案。

其基本原理是使用树结构对全库 item 进行索引，然后训练深度模型以支持树上的逐层检索，从而将大规模推荐中全库检索的复杂度由 O(n) ( n 为所有 item 的量级 ) 下降至 O(log n)。

![img](https://tva1.sinaimg.cn/large/00831rSTly1gdbtp898lwj30kd0bw0ts.jpg)

兴趣树构建：

举例来说，就是要最小化用户行为序列中相近的item-pair在树上的距离。假设用户的行为序列为A -> B -> D -> C，那么我们希望(A,B),(B,D),(D,C)在树上的距离越近越好。两个叶子结点的距离通过其最近的公共祖先确定。

兴趣树搜索：

采用beam-search的方法。