---
layout:     post
title:      推荐系统&知识图谱(1)---DKN
subtitle:   DKN:Deep Knowledge-Aware Network for News Recommendation
date:       2019-09-10
author:     weber
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - recommender systems
    - knowledge graph
    - attention
---
# 0. 简介

最近图网络看的比较多，整理成相关系列，分享写 rs 与 kg 结合的经典paper。

论文：[DKN: Deep Knowledge-Aware Network for News Recommendation](http://xueshu.baidu.com/usercenter/paper/show?paperid=790e73a6cba8bf07cf48e1ba4a018c48&site=xueshu_se&hitarticle=1)，WWW，2018，Microsoft Research Asia

# 1. 应用背景
本文来自微软亚研院，对标的应用场景是微软 Bing News 的新闻推荐。
不同于传统的静态物品推荐，在新闻推荐领域，有三个主要问题：  
**1.新闻文章有高度时间敏感性。**  
如下图所示，新闻的持续时间不长，过时的新闻很快就会被较新的新闻所取代，导致传统的基于ID的协同过滤失效。  

![](https://tva1.sinaimg.cn/large/00831rSTgy1gcvtuvi3o2j30so0q0mzx.jpg)

**2.用户对新闻内容具有明显倾向性。**  
一般一个用户的阅读过的文章属于几个特定的主题，如何利用用户的阅读历史去做推荐是重点所在。  
**3.新闻类文章内容浓缩度高，包含大量实体。**  
如下图所示，用户极有可能选择阅读与历史交互文章具有紧密知识联系的新闻。而以往的模型只停留在字面的语义隐向量和词共现关系上，没有考虑深层的知识层面的关系。

![](https://tva1.sinaimg.cn/large/00831rSTgy1gcvtwknhphj31jw0rs4dg.jpg)

DKN主要就是利用新闻在知识层面的相近度，去发掘用户可能感兴趣的相关新闻。

# 2. 相关知识
## 2.1 Knowledge Graph
知识图谱（Knowledge Graph）于2012年5月17日由Google正式提出，其初衷是为了提高搜索引擎的能力，改善用户的搜索质量以及搜索体验。随着人工智能的技术发展和应用，知识图谱逐渐成为关键技术之一，现已被广泛应用于智能搜索、智能问答、个性化推荐、内容分发等领域。
KG 由实体和边组成，数据形式一般为三元组：$(h,r,t)$。$h$为头结点，表示一个实体；$t$为尾结点，表示一个实体；$r$为两实体间的关系。
## 2.2 Knowledge Graph Embedding
Knowledge Graph Embedding(KGE) 是为 Knowledge Graph(KG) 中的每个实体和关系学习得到一个低维的 Embedding 表示。KGE 学习模型分为两类：基于距离的平移模型(distance-based translational models)和基于语义的匹配模型(semantic-based matching models)。
### 2.2.1 distance-based translational models
基于距离的平移模型使用基于距离的评分函数来评估三元组的概率，将尾结点视为头结点和关系平移得到的结果。代表方法有TransE、TransH、TransR、TransD等。
以TransE为例，简单说明下如何 KG Embedding：  
>**TransE：**
>
>TransE 希望三元组$(h,r,t)$满足$\vec{h} + \vec{r} ≈ \vec{t}$。
因此，TransE 的 Score function 为：$f_r(h,t)=||h + r -t||^2_2$  
>TransE是最简单的，不同的 Embedding 方法主要就是 $f_r(h,t)$ 的定义方式不同。
>
>**感兴趣可以去阅读相关paper：**
>
>TransE，NIPS 2013，[Translating embeddings for modeling multi-relational data]()   
>TransH，AAAI 2014，[Knowledge graph embedding by translating on hyperplanes]()   
>TransA，arXiv 2015，[An adaptive approach for knowledge graph embedding]()    
>TransG，arxiv2015，[A Generative Mixture Model for Knowledge Graph Embedding]()    
>TransR，AAAL 2015，[Learning Entity and Relation Embeddings for Knowledge Graph Completion]()  
>TransD，ACL 2015，[Knowledge graph embedding via dynamic mapping matrix]()

本文作者实验时将各种distance-based的KGE方法进行了实验比较，TransD实验结果最好。

![KGE](https://tva1.sinaimg.cn/large/00831rSTgy1gcvu1bco4oj31yk0dgjv0.jpg)

### 2.2.2 semantic-based matching models
基于语义的匹配模型使用基于相似度的评分函数评估三元组的概率，将实体和关系映射到隐语义空间中，来进行相似度度量。代表方法有MLP、SME、NTN、NAM等。  
## 2.3 Kim CNN
论文：[Convolutional neural networks for sentence classification](http://xueshu.baidu.com/usercenter/paper/show?paperid=7ea81182039becbb82a22aaae8099c15&site=xueshu_se&hitarticle=1)，Kim Y，2014  
Kim CNN是一种从句子中提取 Embedding 的方法，其结构如图所示。

![Kim CNN](https://tva1.sinaimg.cn/large/00831rSTgy1gcvu5ew5hwj31cq0ja7ks.jpg)

**初始输入**：  
$n×k$ 的句子二维矩阵，其中 $n$ 为句子中 word 的个数，$k$ 为word的 embedding 维度(一般使用word2vec预训练值)。  
**卷积**：  
卷积核采用多个不同尺寸的卷积核（类似Inception），举例来说，假设卷积核有3种，取卷积核长度$h$分别为 2,3,4，则三种卷积核尺寸为 $2*k$ , $3*k$ , $4*k$ ，卷机核数量分别取 $n_0$ , $n_1$ , $n_2$ 。
则3种卷积核分别与矩阵做卷积，第一种 $h=2,num=n_0$ 的卷积核，得到 $(n-2+1)*1*n_0$ 的矩阵。  
**池化**：  
$(n-2+1)*1*n_0$ 的矩阵按列池化，得到 $1*n_0$ 的向量，最终将3种卷积结果拼接在一起，得到 $1*(n_0+n_1+n_2)$ 的向量。  
**FC**：  
（如果有需要的话）再将池化结果过一层全连接，变成固定长度的向量。

DKN作者使用Kim CNN把一篇 news 的 title 信息变成了一个定长的向量。
## 2.4 Attention
Attention机制由Google 2017年提出，通俗的讲就是把注意力集中放在重要的点上，网上关于attention的学习资料很多，放了2篇我觉得讲解最为通透的链接在这里。
DKN中主要使用Attention来对不同的emb进行加权求和时的权值计算，利用attention来强调重要的emb的权值。
>参考学习资料：  
>1.[《Attention is All You Need》](https://arxiv.org/abs/1706.03762)，2017，Google  
>2.[ 深度学习中的注意力机制](https://mp.weixin.qq.com/s/tXLCR8H2OjyD6Rsjd7fWEA)，2017，张俊林  
>3.[《Attention is All You Need》浅读（简介+代码）](https://www.spaces.ac.cn/archives/4765)，2018，苏建林   

# 3. DKN模型框架
## 3.1 整体结构
![DKN](https://tva1.sinaimg.cn/large/00831rSTgy1gcvuqdh2xpj31ck0u0wqg.jpg)

DKN(Deep Knowledge-aware Network)的整体模型结构如上图右半部分所示。  
输入分两部分：候选新闻和用户点击过的新闻集合。输入的新闻通过 KCNN 模块转换为对应的embedding，通过Attention层计算 candidate news embedding 和 user's clicked news embedding 之间的attention权重，并将历史新闻embedding加权求和。在顶层拼接两部分向量后，通过MLP来拟合用户点击此条候选新闻的概率。
## 3.2 知识提取(Knowledge Distillation)
知识蒸馏即把一篇 news 对应的 title 文本信息，转为 对应的 embedding。文中使用了3个 embedding：标题中每个单词的word embedding（由word2vec预训练得到），每个单词对应实体的entity embedding，每个实体上下文的context embedding。
### 3.2.1 entity embedding
获取Entity Embedding需要以下四步：

![](https://tva1.sinaimg.cn/large/00831rSTgy1gcvuttvf6xj31lj0u04ae.jpg)

**1. Entity Linking：** 识别出标题中的实体。
>*实体链接相关参考：*  
>[Learning to link with wikipedia](),CIKM 2008  
>[Re-ranking for joint named-entity recognition and linking](),CIKM 2013  
>[百度实体链接比赛后记](https://mp.weixin.qq.com/s/hIGmW_J5xEvLUXa4hFHzsA),2019 苏剑林  

**2. Sub-graph Construction：** 根据标题中识别出的实体集合，得到与实体one hop内的所有实体和关系所构成的子图。  
**3. KGE：** 利用distance-based translational embedding方法如TransD来得到emb。  
**4. Entity mbedding：** 得到标题中单词对应的实体embedding矩阵。
### 3.2.2 context embedding
![](https://tva1.sinaimg.cn/large/00831rSTgy1gcvuvb1hpnj31cw0u0ti0.jpg)

Context embedding即标题中对应实体的上下文实体集的隐向量。通过KG学习得到的entity embedding可以在表示实体信息，但仍有关联信息损失，可以利用实体的上下文去更好的刻画该实体。如上图所示。
因为每个实体的上下文实体有多个，为保证维度相同，取均值：

$$\overline{e} =\frac{1}{|context(e)|}\sum_{e_i\in context(e)}e_i$$

## 3.3 Transformation
在3.2知识抽取部分，我们得到了 word embedding $[w_1\ w_2\ w_3\ ...\ w_n]$，entity embedding $[e_1\ e_2\ e_3\ ... e_n]$，context embedding $[\overline{e}_1\ \overline{e}_2\ \overline{e}_3\ ...\ \overline{e}_n]$，最简单的处理方式是将三者拼接，即$W=[w_1\ w_2\ ...\ w_n\ e_1\ e_2\ ...\ \overline{e}_1\ \overline{e}_2\ ...]$，但这样做有一些限制：
1. 直接拼接破坏了单词和实体对应的联系，不知道它们的对齐方式；
2. 单词embedding和实体embedding是通过不同方法学习到的，不适合在同一向量空间进行计算；
3. 直接拼接需要word与entity隐向量的维度相同，但实际中二者的最优维度可能不同。

因此，作者使用 **transformation function** 对实体emb进行转换，映射到和word emb同一空间：  
transformed entity emb：

$$g(e_{1:n})=[g(e_{1})\ g(e_{2})\ ...\ g(e_{n})]$$

transformed context emb：

$$g(\overline{e}_{1:n})=[g(\overline{e}_{1})\ g(\overline{e}_{2})\ ...\ g(\overline{e}_{n})]$$

$g$ 为转换函数，可以是线性的，也可以是非线性的，即 $g(e)=Me$或$g(e)=tanh(Me+b)$。其中$M\in R^{\ d \times k}$ 是可训练的转移矩阵参数。
转换完的 multi-channel 输入为：

$$W=[w_1\ g(e_{1})\ g(\overline{e}_{1})][w_2\ g(e_{2})\ g(\overline{e}_{2})]...[w_n\ g(e_{n})\ g(\overline{e}_{n})]\in R^{\ d \times n \times 3}$$


## 3.4 KCNN(Knowledge-aware CNN)
![KCNN](https://tva1.sinaimg.cn/large/00831rSTgy1gcvv0urucdj30zv0u0463.jpg)

2.3介绍了Kim CNN，3.3介绍了multi-channel input，KCNN就没什么特别的了。与Kim相同，就是使用多种不同尺寸的卷积核，来提取不同的特征，最后按列pooling。  
所选取的卷积核尺寸 $h \in R ^{\ d \times l\times 3}$，$l$控制不同的卷积窗口大小，设有$m_l$个该卷积核。则每个卷积核的计算结果$c^l_1 \in R^{(n-l+1)\times1}$，总共为$c^l=[c^l_1\ c^l_2\ ...\ c^l_m] \in R^{(n-l+1)\times{m_l}}$，取pooling后为$c^l_{max}=\max\{c_1^l,c_2^l,...,c^l_{n-l+1}\}\in R^{1 \times{m_l}}$。
多个尺寸的卷积核，最终pooling结果向量长度为 $m_{l_1}+m_{l_2}+...+m_{l_z}$，其中z为不同尺寸卷积核的种类数。
## 3.5 Attention Net
![attention](https://tva1.sinaimg.cn/large/00831rSTgy1gcvv1pqlr1j31bm0rodk3.jpg)

使用KCNN获取到用户点击过的每篇新闻的向量表示以后，作者并没有简单地将emb做 sum/avg 来代表该用户，而是计算候选文档对于用户每篇点击文档的 attention，利用attention作为权值，对用户所有点击过的新闻的emb加权求和，从而改变不同历史新闻对用户影响的大小。  

**具体做法：**  
1.concat：将候选新闻的 emb 和一篇历史新闻的 emb 拼接起来，得到DNN输入；  
2.DNN：将输入通过一个MLP，将所有历史新闻输出值的 softmax 作为权值；  
3.加权求和：将所有历史新闻 emb 和对应的 softmax 权值相乘后求和，得到 user emb。
#4. 实验结果
## 4.1 实验数据
交互数据用的是 Bing News 的用户点击日志，包含用户id，新闻url，新闻标题，点击与否（0/1）。2016年10月16日到2017年7月11号的数据作为训练集，2017年7月12号到8月11日的数据作为测试集。
知识图谱数据用的是 [Microsoft Satori](https://searchengineland.com/library/bing/bing-satori)。

![](https://tva1.sinaimg.cn/large/00831rSTgy1gcvv7o1sbpj318o0gw791.jpg)
## 4.2 实验结果
![](https://tva1.sinaimg.cn/large/00831rSTgy1gcvv8o0uyzj31350u04b3.jpg)
![](https://tva1.sinaimg.cn/large/00831rSTgy1gcvv92oubij314q0toqbr.jpg)
实验表明，在使用DKN模型时，同时使用实体embedding和上下文embedding、使用TransD方法、使用非线性变换、使用attention机制可以获得更好的预测效果。
# 5. DKN总结
优点：  
1. 解决了新闻推荐的三大难题
2. 创新性的使用知识图谱的上下文实体去扩展的实体
3. 将不同空间的向量使用多通道拼接，使用CNN去过滤
4. 用attention去得到用户历史交互新闻的权值，使用加权求和而不是简单的均值
   
缺点：
1. DKN是基于内容的，entity emb需要提前训练获得，导致DKN不是end-to-end训练。
2. DKN很难融合除了文字以外的其他边信息。