---
layout:     post
title:      经典CTR模型(9)---DIN
subtitle:   Deep Interest Network for Click-Through Rate Prediction
date:       2020-01-09
author:     weber
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - recommender systems
    - CTR
---

论文：[Deep Interest Network for Click-Through Rate Prediction](http://xueshu.baidu.com/usercenter/paper/show?paperid=b623eda9a09041b1d61e39c9b81b4ac1&site=xueshu_se)，KDD，2018

# 1. 背景

深度学习在CTR预估领域已经有了广泛的应用，常见的算法比如Wide&Deep，DeepFM等。这些方法一般的思路是：通过Embedding层，将高维离散特征转换为固定长度的连续特征，然后通过多个全联接层，最后通过一个sigmoid函数转化为0-1值，代表点击的概率。即**Sparse Features -> Embedding Vector -> MLPs -> Sigmoid -> Output**.

但阿里的研究者们通过观察收集到的线上数据，发现了用户行为数据中有两个很重要的特性：

1. **Diversity**：用户在浏览电商网站的过程中显示出的兴趣是十分多样性的。
2. **Local activation**: 由于用户兴趣的多样性，只有部分历史数据会影响到当次推荐的物品是否被点击，而不是所有的历史记录。 

针对上面提到的用户行为中存在的两种特性，阿里将其运用于自身的推荐系统中，推出了深度兴趣网路DIN。

# 2. DIN结构

DIN的结构如图所示：EMbedding Layer -> Attention Network -> MLP

![image-20200323162441625](https://tva1.sinaimg.cn/large/00831rSTgy1gd3xuiiqyyj31i80t8qdn.jpg)

这里重点说明一下 DIN 中的注意力机制。

## 2.1 Attention 

Attention机制简单的理解就是，针对不同的广告，用户历史行为与该广告的权重是不同的。假设用户有ABC三个历史行为，对于广告D，那么ABC的权重可能是0.8、0.1、0.1；对于广告E，那么ABC的权重可能是0.3、0.6、0.1。这里的权重，就是Attention机制即上图中的Activation Unit所需要学习的。

得到每个用户的历史行为的 权值后，对用户不定长的历史行为进行加权后pooling，得到embedding维度的定长向量。再和其他的dense feature，sparse feature 拼接起来，过MLP。

> 细节1：attention不经过softmax
>
> DIN 原论文4.3节说 attention 的输入的权重不用经过 softmax，为了保留用户兴趣的强度。

> 细节2：attention实现时，过attention net的为4个input
>
> 在具体实现时，先把query扩充到keys长度，得到querys。 除了querys，keys以外，还使用了 querys-keys，querys*keys作为attention net的输入，算是额外构建的相似度特征。

# 3. 其他创新点

### 3.1  Dice激活函数

尽管对Relu进行了修正得到了PRelu，但是仍然有一个问题，即我们认为分割点都是0，但实际上，分割点应该由数据决定，因此文中提出了Dice激活函数，全称是Data Dependent Activation Function。

Dice激活函数的全称是Data Dependent Activation Function。

### 3.2  Adaptive Regularization

CTR中输入稀疏而且维度高，通常的做法是加入L1、L2、Dropout等防止过拟合。但是论文中尝试后效果都不是很好。用户数据符合长尾定律long-tail law，也就是说很多的feature id只出现了几次，而一小部分feature id出现很多次。这在训练过程中增加了很多噪声，并且加重了过拟合。

对于这个问题一个简单的处理办法就是：直接去掉出现次数比较少的feature id。但是这样就人为的丢掉了一些信息，导致模型更加容易过拟合，同时阈值的设定作为一个新的超参数，也是需要大量的实验来选择的。

因此，阿里提出了**自适应正则**的做法，即：
 1.针对feature id出现的频率，来自适应的调整他们正则化的强度；
 2.对于出现频率高的，给与较小的正则化强度；
 3.对于出现频率低的，给予较大的正则化强度。

