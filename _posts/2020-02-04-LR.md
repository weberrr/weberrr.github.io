---
layout:     post
title:      统计学习方法(4)---LR
subtitle:   Logistic Regression，LR
date:       2020-02-04
author:     weber
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - machine learning
---

Logistic Regression，LR

# 1. 一句话介绍LR

逻辑斯谛回归假设数据服从伯努利**分布**，通过极大似然函数的**方法**，运用梯度下降求解**参数**，达到将数据二**分类**的目的。

关键词：伯努利分布->极大似然函数->随机梯度下降->分类方法

其表达式为：

$$
\hat y = \frac{1}{1+e^{-\theta^Tx}}
$$

根据预测值与给定阈值的大小，来判断是否为正类，是个分类模型。

# 2. 损失 & 梯度推导

loss：

$$
loss = -\sum_{i=1}^N \Big ( y_ilog(\hat y_i) + (1-y_i)log(1-\hat y _i) \Big) = \sum_{i=1}^N \Big( y_i \theta^Tx_i - log(1+e^{\theta^T x_i}) \Big)
$$

一个样本的梯度：

$$
\frac{\partial}{\partial \theta} l = (y_i-\hat y_i) x_i
$$

可采用随机梯度下降法，并行更新。

# 3. 面试问题总结

> LR的优点？

1. 简单，可解释性强；
2. 容易并行，训练速度快；

> LR的缺点？

1. 难以处理非线性数据；
2. 无法筛选特征；

> 为什么采用极大似然函数作为损失？

我们希望每一个样本的预测都得到最大的概率，所有样本相乘后的结果最大，即极大似然函数；

> 为什么不适用MSE作为损失？

1. 使用MSE时是非凸函数，不易优化，易局部最小。

2. 梯度更新速度会与sigmoid函数的梯度有关，会导致训练速度很慢。

> 如果存在线性相关的特征如何处理？

一般不会影响分类器效果，但是会降低可解释性（线性相关的特征的权值不再能反应特征重要性）。

> LR如何并行？

与batch的梯度一样，可以将数据切分，并行计算梯度。

> LR与最大熵的关系？

逻辑回归跟最大熵模型**没有本质区别**。逻辑回归是最大熵对应类别为**二类**时的特殊情况，也就是当逻辑回归类别扩展到**多类**别时，就是最大熵模型。

> 怎么防止过拟合？为什么？

添加L1/L2正则化，正则化则是对模型参数添加先验，使得模型复杂度较小，对于噪声扰动相对较小。

# 参考资料

[关于逻辑回归，面试官们都怎么问](https://mp.weixin.qq.com/s/Mdn9yiT20oFhyuFLyd6YnA)

[LR为什么用极大似然估计，损失函数为什么是log损失函数（交叉熵）](https://www.jianshu.com/p/cb60d5296a11)

[Logistic Regression常见面试题整理](https://zhuanlan.zhihu.com/p/34670728)

