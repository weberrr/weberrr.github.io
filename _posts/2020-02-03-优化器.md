---
layout:     post
title:      optimizer优化器总结
subtitle:   SGD,Adagrad,RMSProp,Adam
date:       2020-02-03
author:     weber
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - deep learning
    - summary
---

深度学习的优化目标都是最小化目标函数，一般过程为求解目标函数的梯度$\bigtriangledown J(\theta)$，然后将参数 $\theta$ 沿着负梯度方向更新：$\theta^{(t)} = \theta^{(t-1)} - \bigtriangledown J(\theta)$。本文对`tf.keras.optimizer`中实现的常用的优化器进行总结和比较。

# 1. SGD

现在的随机梯度下降（Stochastic Gradient Descent，SGD）一般都是指 mini-batch gradient descent。SGD学习率 $\eta$ 不变，每次计算一个batch内的数据的梯度并进行更新。

SGD的梯度下降过程，类似于一个小球从山坡上滚下，它的前进方向只于当前山坡的最大倾斜方向一致(最大负梯度方向)，每一个时刻的初速度为０。

### 1.1 公式

$$
\theta^{(t)} = \theta^{(t-1)} - \eta \bigtriangledown_{\theta}J(\theta^{(t-1)};x^{(i:i+n)};y^{(i:i+n)})
$$

###1.2 缺点

1. 选择合适的learning rate比较困难。太小的话收敛速度慢，太大的话会使loss在最小值附近震荡。
2. learning rate对于所有的参数（特征）是相同的。如果我们的数据稀疏，我们希望对于不常出现的特征进行大一点的更新，对于常出现的特征更新的慢一些。
3. SGD容易收敛到局部最优，在某些情况下可能被困在鞍点。

### 1.3 改进1 - Momentum

Momentum 在计算梯度时，加入前一阶段的梯度值，克服SGD易震荡的缺点 ：
$$
\begin{aligned}
v^{(t)} =& \gamma v^{(t-1)} + \eta \bigtriangledown_{\theta}J(\theta) 
\\
\theta^{(t)} = &\theta^{(t-1)} - v^{(t)}
\end{aligned}
$$
SGD Momentum的梯度下降过程，类似于一个小球从山坡上滚下，它的前进方向由当前山坡的最大倾斜方向与之前的下降方向共同决定，小球具有初速度(动量)。

![img](https://tva1.sinaimg.cn/large/00831rSTgy1gd9em52zq1j30jg04r3zs.jpg)

### 1.3 改进2 - Nesterov

Nesterov Accelerated Gradient 在 Momentum 的基础上进行改进。在计算梯度时，不是在当前位置，而是未来的位置上：
$$
\begin{aligned}
v^{(t)} =& \gamma v^{(t-1)} + \eta \bigtriangledown_{\theta}J(\theta - \gamma v^{(t-1)})
\\
\theta^{(t)} = &\theta^{(t-1)} - v^{(t)}
\end{aligned}
$$
NAG的梯度下降过程，类似于小球每次先根据上一时刻的初速度移动一个小位置，然后在新的位置滚下，它的前进方向由当前山坡的最大倾斜方向与之前的下降方向共同决定，小球具有初速度(动量)。

### 1.4 tf.keras.optimizer

```python
tf.keras.optimizers.SGD(
    learning_rate=0.01, momentum=0.0, nesterov=False, name='SGD', **kwargs
)
```

目前为止，我们可以做到，**在更新梯度时顺应 loss function 的梯度来调整速度，并且对 SGD 进行加速**。

**我们还希望可以根据参数的重要性而对不同的参数进行不同程度的更新。**

# 2. Adagrad

自适应梯度算法（Adaptive Gradient Algorithm，Adagrad）通过以往的梯度自适应更新学习率 $\eta$，使得不同的 $\theta_i$ 具有不同的学习率：常出现的特征学习率低，不常出现的特征学习率高，比SGD更容易处理稀疏数据。

### 2.1 公式

$$
\begin{aligned}
\theta_i^{(t)} =&\ \theta_i^{(t-1)} - \frac{\eta}{\sqrt{G^{(t)}_{ii}+\epsilon}} \bigtriangledown_{\theta}J(\theta_i)
\end{aligned}
$$

其中，G为对角矩阵：
$$
G^{(t)}_{ii} = G^{(t-1)}_{ii} +(\bigtriangledown_{\theta}J(\theta_i))^2
$$

### 2.2 缺点

分母会不断积累，学习率会收缩，最终变得非常小。

### 2.3 tf.keras.optimizer

```python 
tf.keras.optimizers.Adagrad(
    learning_rate=0.001, initial_accumulator_value=0.1, epsilon=1e-07,
    name='Adagrad', **kwargs
)
```

# 3. RMSProp

RMSProp与Adadelta类似，都是解决Adagrad梯度急速下降问题的。RMSProp与AdaGrad不同的地方在于，不是直接对平方梯度进行累加，而是添加了一个衰减系数来控制历史信息获取的多少。

### 3.1 公式

$$
\theta^{(t)} = \theta^{(t-1)} - \frac{\eta}{\sqrt{E[g^2]^{(t-1)}+\epsilon}} g^{(t)}
$$

其中，$g^{(t)}=\bigtriangledown_{\theta}J(\theta)$，$E[g^2]^{(t)} = \gamma E[g^2]^{(t-1)} +(1-\gamma) (g^2)^{(t)} = \sum_{i=0}^t \gamma^i(g^2)^{(t-i)}$。通常$\gamma=0.9$。

### 3.2 解释

如图绿线所示，**在参数空间更为平缓的方向，会取得更大的进步（因为平缓，所以历史梯度平方和较小，对应学习率下降的幅度较小），并且能够使得陡峭的方向变得平缓，从而加快训练速度。**

![img](https://tva1.sinaimg.cn/large/00831rSTgy1gd9hjfvwb1j314007kgmb.jpg)

### 3.3 tf.keras.optimizer

```python
tf.keras.optimizers.RMSprop(
    learning_rate=0.001, rho=0.9, momentum=0.0, epsilon=1e-07, centered=False,
    name='RMSprop', **kwargs
)
```


# 4. Adadelta

Adadelta与RMSProp类似，也是对Adagrad的改进。相较于RMSProp，Adadelta还提供了不用提供初始学习率 $\eta$ 的思路。

### 4.1 公式

$$
\theta^{(t+1)} = \theta^{(t)} - \frac{RMS[\bigtriangleup \theta]^{(t-1)}}{RMS[g]^{(t)}}g^{(t)}
$$

其中，分母的形式与RMSProp一致，只是换用均方根（root mean squared，RMS）进行表示。主要对分子进行说明：
$$
RMS[\bigtriangleup \theta]^{(t)} = \sqrt{E[\bigtriangleup \theta^2]^{(t)} + \epsilon}
$$

$$
E[\bigtriangleup \theta^2]^{(t)} = \gamma E[\bigtriangleup \theta^2]^{(t-1)} + (1- \gamma)  (\bigtriangleup \theta^2)^{(t)}
$$

可以看出，是对 $\theta^2$的变化量的指数加权平均，通过前一时刻的变化量来估计学习率。

### 4.2 tf.keras.optimizer

```python
tf.keras.optimizers.Adadelta(
    learning_rate=0.001, rho=0.95, epsilon=1e-07, name='Adadelta', **kwargs
)
```

# 5. Adam

Adam（Adaptive Moment Estimation）相当于 RMSProp + Momentum

### 5.1 公式

和 RMSProp 和 Adadelta 一样，Adam使用了过去梯度平方的指数衰减平均值：
$$
v^{(t)} = \beta_2 v^{(t-1)} + (1-\beta_2) (g^2)^{(t)}
$$
但初始化为0会导致$v^{(t)}$ 在初期时偏向于0，所以进行偏差纠正：
$$
\hat v^{(t)} = \frac{v^{(t)}}{1-\beta_2^{(t)}}
$$
除了学习率，Adam也对过去梯度计算了指数衰减平均值：
$$
m^{(t)} = \beta_1m^{(t-1)}+(1-\beta_1)g^{(t)}
$$
同样，进行了偏差纠正：
$$
\hat m^{(t)}=\frac{m^{(t)}}{1-\beta_1^{(t)}}
$$
最终梯度更新规则：
$$
\theta^{(t+1)} = \theta^{(t)} - \frac{\eta}{\sqrt{ \hat v^{(t)}+\epsilon}} \hat m^{(t)}
$$
默认，$\beta_1=0.9，\beta_2=0.999，\epsilon=10e-8$

### 5.2  tf.keras.optimizer

```python
tf.keras.optimizers.Adam(
    learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,
    name='Adam', **kwargs
)p
```

# 6. Adamax

Adam的变体，在Adam中使用 l2 norm 度量梯度。

### 6.1 公式

可以将Adam中的梯度一阶式推广为 l2 norm 的形式：

$$
v^{(t)}=\beta_2^p v^{(t-1)} + (1-\beta_2^p)(|g|^{p})^{(t)}
$$

在 Adamax中，使用无穷范数度量梯度大小，用于收敛到更稳定的状态：
$$
v^{(t)}=\beta_2^\infty v^{(t-1)} + (1-\beta_2^\infty )(|g|^{\infty })^{(t)} = max(\beta_2 v^{(t-1)},|g|^{(t)})
$$
梯度更新式：
$$
\theta^{(t+1)} = \theta^{(t)} - \frac{\eta}{v^{(t)}}\hat m^{(t)}
$$

### 6.2  tf.keras.optimizer

```python
tf.keras.optimizers.Adamax(
    learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name='Adamax',
    **kwargs
)
```

# 7. Nadam

**Nadam**(Nesterov-accelerated Adaptive Moment Estimation) 即 Adam与Nesterov的组合。

### 7.1 公式

$$
\theta^{(t+1)} = \theta^{(t)} - \frac{\eta}{\sqrt{ \hat v^{(t)}+\epsilon}} (\hat m^{(t)}+\frac{1-\beta_1}{1-\beta_1^{(t)}}g^{(t)})
$$

### 7.2 tf.keras.optimizer

```python
tf.keras.optimizers.Nadam(
    learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name='Nadam',
    **kwargs
)
```

# 8. 优化器对比

![img](https://tva1.sinaimg.cn/large/00831rSTgy1gd9p0ejsmug30h80dc7k9.gif)

可以看出，Adagrad, Adadelta, RMSprop 几乎很快就找到了正确的方向并前进，收敛速度也相当快，而其它方法要么很慢，要么走了很多弯路才找到。

### 8.1 选择

如果数据稀疏，应采用自适应方法，Adam， Adagrad, Adadelta, RMSprop。

**随着梯度变的稀疏，Adam 比 RMSprop 效果会好。**

整体来讲，**Adam 是最好的选择**。

很多论文里都会用 SGD，没有 momentum 等。**SGD 虽然能达到极小值，但是比其它算法用的时间长，而且可能会被困在鞍点**。

如果需要更快的收敛，或者是训练更深更复杂的神经网络，需要用一种自适应的算法。

# 参考资料

[优化器optimizer详解](https://www.cnblogs.com/guoyaohua/p/8542554.html)

[深度学习优化器总结](https://zhuanlan.zhihu.com/p/58236906)

[深度学习最全优化方法总结比较](https://zhuanlan.zhihu.com/p/22252270)

[tf.keras.optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)