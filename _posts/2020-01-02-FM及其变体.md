---
layout:     post
title:      经典CTR模型(2)---FM及其变体
subtitle:   FM，FFM，AFM
date:       2020-01-02
author:     weber
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - recommender systems
    - CTR
---

[Factorization Machines](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf)，2010

[Field-aware Factorization Machines for CTR Prediction](https://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf)，2016，RecSys

[Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks](https://arxiv.org/pdf/1708.04617.pdf)，2017，IJCAI

# 1. FM

FM（Factorization Machines）是在2010年提出的一种学习二阶特征交叉的模型。在原先线性模型的基础上，枚举了所有二阶交叉特征融入模型。
$$
f(x) = w_0 + \sum_{i=1}^nw_ix_i+\sum_{i=1}^n\sum_{j=i+1}^n<v_i,v_j>x_ix_j
$$

> 注意到FM的二阶交叉项系数并没有使用 $w_{ij}$，而是使用$<v_i,v_j>$，这样做的好处：
>
> 1. 每次更新 $v_i$ 的向量，都会影响和特征 i 相交叉的参数，使得特征权重学习不再相互独立；
> 2. train set 中没出现过的交叉特征，也可以给出一个非0预估。

FM的参数梯度：

$$
\begin{equation}  
\frac{\partial}{\partial \theta}\hat y(x)=\left\{  
             \begin{array}{**lr**}  
             1, &  if\ \theta\ is \ w_o \\  
             x_i, & if\ \theta\ is \ w_i \\  
             x_i\sum_{j=1}^nv_{j,f}x_j-v_{j,f}x_i^2, &     if\ \theta\ is \ v_{i,f} 
             \end{array}  
\right.  
\end{equation}
$$

FM优点：

1. 有效处理稀疏场景下的特征学习；
2. 具有线性时间复杂度：$ab =\frac{1}{2}[(a+b)^2-(a^2+b^2)]$；
3. 能够对训练集中未出现的特征组合进行泛化；

FM缺点：

1. 仅考虑了二阶，没考虑高阶；

# 2. FFM

FFM（Field-aware Factorization Machine）是Yuchin Juan等人在2015年的比赛中提出的一种对FM改进算法，主要是引入了field概念，即认为每个feature对于不同field的交叉都有不同的特征表达。FFM相比于FM的计算时间复杂度更高，但同时也提高了本身模型的表达能力。

# 3. AFM

FM枚举了所有二阶交叉特征，但实际上有些交叉特征与我们的预估目标关联不大，AFM使用attention机制来学习不同二阶特征的重要性（这个思路与FFM一致，引入额外信息来表达不同特征交叉的重要性）

![image-20200323092237492](https://tva1.sinaimg.cn/large/00831rSTgy1gd3lngwi3mj31e60kate7.jpg)
  
$$
f(x) = w_0 + \sum_{i=1}^nw_ix_i+p^T\sum_{i=1}^n\sum_{j=i+1}^n\alpha_{ij}(v_i\odot v_j)x_ix_j
$$

其中,$\alpha_{ij}=\frac{\exp(e_{ij})}{\sum_{i,j}\exp(e_{ij})}$，$e_{ij}=h^TReLU(W(v_i \odot v_j )x_ix_j + b)$

优点：

1. 引入attention机制赋予不同交叉特征不同的重要程度，增加了一定可解释性。

缺点：

1. 仍然是浅层模型，没有学习到高阶的特征。

# 4.FM基于Pytorch代码实现

```python
class FM(nn.Module):
    """
      Input shape
        - 3D tensor with shape: ``(batch_size,field_size,embedding_size)``.
      Output shape
        - 2D tensor with shape: ``(batch_size, 1)``.
    """

    def __init__(self):
        super(FM, self).__init__()

    def forward(self, inputs):
        fm_input = inputs

        square_of_sum = torch.pow(torch.sum(fm_input, dim=1, keepdim=True), 2)
        sum_of_square = torch.sum(fm_input * fm_input, dim=1, keepdim=True)
        cross_term = square_of_sum - sum_of_square
        cross_term = 0.5 * torch.sum(cross_term, dim=2, keepdim=False)

        return cross_term
```

