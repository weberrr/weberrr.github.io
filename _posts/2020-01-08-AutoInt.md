---
layout:     post
title:      经典CTR模型(8)---AutoInt
subtitle:   AutoInt Automatic Feature Interaction Learning via Self-Attentive Neural Networks
date:       2020-01-08
author:     weber
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - recommender systems
    - attention
    - CTR
---

论文：[AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks](http://xueshu.baidu.com/usercenter/paper/show?paperid=1n4t0pk0y9720xy08v0p0030qn161587&site=xueshu_se)，CIKM，2019

# 1. 论文背景

AutoInt是2019年发表的比较新的论文，它的思路和DCN以及xDeepFM相似，都是提出了能够显式学习高阶特征交叉的网络。除此之外，AutoInt算法借鉴了NLP模型中Transformer的Multi- head self-attention机制，给模型的交叉特征引入了可解释性，可以让模型知道哪些特征交叉的重要性更大。

# 2. AutoInt结构

相比于DCN和xDeepFM采用交叉网络+DNN的双路结构，AutoInt直接采用了单路的模型结构，将原始特征Embedding之后，直接采用多层Interacting Layer进行学习（作者在论文的实验部分也列出了AutoInt+DNN的双路模型结构：AutoInt+）。

![image-20200323154103237](https://tva1.sinaimg.cn/large/00831rSTly1gd3wl3qrdrj30ze0p041i.jpg)

主要讲述下其创新模块：Interacting Layer

## 2.1 Interacting Layer

AutoInt中的Interacting Layer包含了两部分：Multi-head Self-Attention和ResNet部分。

在self-attention中，采用的是Q,K,V形式，具体来说：我们只考虑1个head self-attention的情况，假设我们共有  M 个特征，对于输入的第 m 个feature embedding来说，AutoInt认为它与  M 个特征交叉后的特征拥有不同的权重，对于我们第 m 个特征，它与第 k  个特征交叉的权重为：
$$
\alpha_{m,k}=\frac{\exp(\phi(e_m,e_k))}{\sum_{l=1}^M\exp(\phi(e_m,e_k))}
$$
其中，$\phi(e_m,e_k) = <W_{query}e_m,W_{key}e_k>$，函数$\phi$本文采用的是内积。

得到权重信息后，我们对M个特征的Value进行加权：
$$
\tilde e_{m} = \sum_{k=1}^M \alpha_{m,k}(W_{Value} e_k)
$$
得到向量m与其余特征的加权二阶交叉信息。

作者使用了多个self-attention（multi-head self- attention）来计算不同subspaces中的特征交叉，其实就是进一步增加了模型的表达能力。采用h个multi-head之后，我们会得到h个 $\tilde e_m^{(h)}$ ，将这h个 concat起来，得到：
$$
\tilde e_m = [\tilde e_m^{(1)},\tilde e_m^{(2)},...,\tilde e_m^{(h)}]
$$
为了保留上一步学到的交叉信息，使用ResNet思想，使得之前学习到的信息也被更新到新的层中，：
$$
e_m^{Res} = ReLU(\tilde e_m + W_{Res}e_m)
$$

对每个特征得到加权后的向量表示后，最终输出：
$$
\hat y = \sigma (w^T(e_1^{Res} \oplus e_2^{Res}  \oplus ···\oplus e_M^{Res}) + b)
$$

# 3. 优缺点

优点：

1. AutoInt可以显示地、以vector-wise的方式地学习有限阶（bounded-degree）特征交叉信息
2. 引入注意力机制，增加可解释性；

# 4. 阶段性总结

这里引用知乎的图，对前几篇经典的ctr预估paper进行总结。

![preview](https://tva1.sinaimg.cn/large/00831rSTgy1gd3xauidtdj317c0u0gt6.jpg)

![img](https://tva1.sinaimg.cn/large/00831rSTgy1gd3xb9hjo9j30k0077dha.jpg)

从上往下，代表了整个CTR预估的发展趋势：

- **LR的主要限制在于需要大量手动特征工程来间接提高模型表达，此时出现了两个发展方向：**

- - 以FM为代表的端到端的隐向量学习方式，通过embedding来学习二阶交叉特征
  - 以GBDT+LR为代表的两阶段模型，第一阶段利用树模型优势自动化提取高阶特征交叉，第二阶段交由LR进行最终的学习

- **以FM为结点，出现了两个方向：**

- - 以FFM与AFM为代表的浅层模型改进。这两个模型本质上还是学习低阶交叉特征，只是在FM基础上为不同的交叉特征赋予的不同重要度
  - 深度学习时代到来，依附于DNN高阶交叉特征能力的Embedding+MLP结构开始流行

- **以Embedding+MLP为结点：**

- - Embedding层的改造+DNN进行高阶隐式学习，出现了以PNN、NFM为代表的product layer、bi-interaction layer等浅层改进，这一类模型都是对embedding层进行改造来提高模型在浅层表达，减轻后续DNN的学习负担
  - 以W&D和DeepFM为代表的双路模型结构，将各个子模块算法的优势进行互补，例如DeepFM结合了FM的低阶交叉信息和DNN的高阶交叉信息学习能力
  - 显式高阶特征交叉网络的提出，这一阶段以更复杂的网络方式来进行显式交叉特征的学习，例如DCN的CrossNet、xDeepFM的CIN、AutoInt的Multi-head Self-attention结构

从整个宏观趋势来看，每一阶段新算法的提出都是在不断去提升模型的表达能力，从二阶交叉，到高阶隐式交叉，再到如今的高阶显示交叉，模型对于原始信息的学习方式越来越复杂的同时，也越来越准确。