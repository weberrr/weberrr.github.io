---
layout:     post
title:      统计学习方法---CRF
subtitle:   CRF
date:       2020-02-07
author:     weber
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - machine learning
---

conditional random field，CRF

# 1. 一句话概括CRF

CRF是给定一组输入序列条件下另一组输出序列的条件分布模型。

## 1.1 线性链条件随机场

随机场：若干个位置组成的整体，当给每个位置按某种分布赋予一个值后，其全体就叫随机场。

马尔可夫随机场：每个位置的赋值仅仅与其相邻的位置的赋值有关。

条件随机场：在给定随机变量X的条件下，随机变量Y的马尔可夫随机场，在词性标注时，X为词，Y为词性。

**线性链条件随机场**：

设$𝑋=(𝑋_1,𝑋_2,...𝑋_𝑛),𝑌=(𝑌_1,𝑌_2,...𝑌_𝑛)$均为线性链表示的随机变量序列，在给定随机变量序列$X$的情况下，随机变量$Y$的条件概率分布$P(Y|X)$构成条件随机场，即满足马尔科夫性：
$$
𝑃(𝑌_𝑖|𝑋,𝑌_1,𝑌_2,...𝑌_𝑛)=𝑃(𝑌_𝑖|𝑋,𝑌_{𝑖−1},𝑌_{𝑖+1})
$$

![条件随机场定义](https://tva1.sinaimg.cn/large/00831rSTgy1gdivm02baxj30ku055gmz.jpg)

## 1.2 三种表示方式

### 1.2.1 参数形式

$$
P(y|x) = \frac{1}{Z(x)} \exp (\sum_{i,k}\lambda_kt_k(y_{i-1},y_i,x,i)+\sum_{i,l}\mu_l s_{l}(y_{i},x,i)
$$

其中，$t_k(y_{i-1},y_i,x,i)$为定义在边上的转移特征，依赖于当前时刻和前一时刻的位置，K为该结点的转移特征函数的总个数；$s_l(y_i,x,i)$为定义在结点上的状态特征，依赖于当前位置，L为该结点的状态特征函数的总个数。Z(x)为所有序列Y的和，即归一化因子。

### 1.2.2 简化形式

$$
P(y|x) = \frac{1}{Z(x)} \exp(\sum_{k=1}^K(w_kf_k(y,x)))
$$

其中，
$$
w_k = \left\{
\begin{array}{}
\lambda_k,  k=1,2,...K_1\\
\mu_l, k=K_2+1,K_1+2,...K
\end{array}
\right.
$$

$$
f_k(y_,x) = \sum_{i=1}^n(f_k(y_{i-1},y_i,x,i))
$$

其中，
$$
f_k(y_{i-1},y_i,x,i) = \left\{
\begin{array}{}
t_k(y_{i-1},y_i,x,i),  k=1,2,...K_1\\
s_k(y_{i},x,i), k=K_2+1,K_1+2,...K
\end{array}
\right.
$$

### 1.2.3 矩阵形式

$$
P(y|x) =\frac{1}{Z_w(x)} \prod_{i=1}^{n+1}[M_i(y_{i-1},y_i|x)]
$$

其中，

$$
M_i(y_{i-1},y_i|x) = \exp (W_i(y_{i-1},y_i,x)) = \exp(\sum_{k=1}^K(w_kf_k(y_{i-1},y_i,x,i)))
$$

有矩阵 

$$
M_i(x) = [M_i(y_{i-1},y_i|x)]_{m \times m}
$$

# 2. 三个问题

## 2.1 概率计算

已知输入序列x，输出序列y，模型参数w，特征函数fk

1. 求解条件概率$P(Y_i=y_i｜x)$ 和$P(Y_{i-1}=y_{i-1},Y_i=y_i｜x)$
2. 联合分布和条件分布的数学期望。

### 2.1.1 前向后向算法

定义

$$
\alpha_0(y|x) =\left\{
\begin{array}{}
1 , y =start\\
0, not
\end{array}
\right.
$$

$$
\alpha_i(y_i|x) = a_{i-1}(y_{i-1}|x)[M_{i}(y_{i-1},y_i,x)]
$$

其中，$[M_{i}(y_{i-1},y_i,x)]$为从 $y_{i-1}$ 转移到 $y_i$ 的非规范化概率。

假设 yi 的可能取值有 m 种，则：

$$
a_i^T(x) = a_{i-1}^T(x) M_i{(x)}
$$

同理，有

$$
\beta_i(x) = M_{i+1}(x)\beta_{i+1}(x)
$$

可以得到

$$
Z(x) = a^T_n(x) \mathbf{1} = \mathbf{1}\beta_n^T(x)
$$

### 2.1.2 条件概率

$$
P(Y_i=y_i|x) = \frac{\alpha_i(y_i|x)\beta_i(y_i|x)}{Z_(x)}
$$

$$
P(Y_{i-1}=y_{i-1},Y_i=y_i|x) = \frac{\alpha_{i-1}(y_{i-1}|x)M_i(y_{i-1},y_i|x)\beta_i(y_i|x)}{Z(x)}
$$

### 2.1.3 数学期望

得到了条件概率，很好求解联合分布和条件分布的数学期望。

已知特征函数为：
$$
f_k(y_,x) = \sum_{i=1}^n(f_k(y_{i-1},y_i,x,i))
$$
其关于P(y|x)的数学期望：
$$
E_{P(y|x)}[f_k] = \sum_{y}P(y|x)f_k(y,x) = \sum\limits_{y}P(y|x) \sum\limits_{i=1}^{n+1}f_k(y_{i-1},y_i,x, i) 
$$
同理，P(y,x)的期望：
$$
\begin{aligned}
E_{P(x,y)}[f_k] &
= \sum\limits_{x,y}P(x,y) \sum\limits_{i=1}^{n+1}f_k(y_{i-1},y_i,x, i)\\
& = \sum\limits_{x}\overline{P}(x) \sum\limits_{y}P(y|x) \sum\limits_{i=1}^{n+1}f_k(y_{i-1},y_i,x, i) 
\end{aligned}
$$

## 2.2 学习

描述：给定训练数据集X，对应标记序列Y，K个特征函数 fk(y,x)，学习参数wk，并求条件概率Pw(y｜x)

输入：特征函数$f_k(y_{i-1},y_i,x,i)$，观测序列 $x=(x_1,x_2,...,x_n)$，状态序列 $y=(y_1,y_2,...,y_n)$，

输出：权值 $\hat w_k$，模型$P_w(y｜x)$

思路：使用随机梯度下降来学习。

1. 条件概率满足条件：

$$
P_w(y|x) = \frac{1}{Z_w(x)} \exp (\sum_{k=1}^Kw_kf_k(y,x)) = \frac{ \exp (\sum_{k=1}^Kw_kf_k(y,x))}{\sum_y \exp (\sum_{k=1}^Kw_kf_k(y,x))}
$$

2. 极大似然：

$$
L(w) = \log \prod_{x,y} P_w(y|x)^{\overline P(x,y)} = \sum_{x,y} \overline P(x,y) \log P_w(y|x)
$$

3. 转化为损失：

$$
\begin{aligned}
f(w) & = -L(w) \\
 & = \sum_{x,y}\overline P(x,y)\log(Z_w(x)) - \sum_{x,y}\overline P(x,y)\sum _{k=1}^Kw_kf_k(x,y)\\
& =  \sum_{x}\overline P(x)\log(Z_w(x)) - \sum_{x,y}\overline P(x,y)\sum _{k=1}^Kw_kf_k(x,y)\\
& =  \sum_{x}\overline P(x)\log(\sum_y \exp \sum _{k=1}^Kw_kf_k(x,y)) - \sum_{x,y}\overline P(x,y)\sum _{k=1}^Kw_kf_k(x,y)
\end{aligned}
$$

4. 求梯度：	

$$
\frac{\partial f(w)}{\partial w} = \sum\limits_{x,y}\overline{P}(x)P_w(y|x)f(x,y) -  \sum\limits_{x,y}\overline{P}(x,y)f(x,y)
$$

## 2.3 解码

输入：特征函数$f_k(y_{i-1},y_i,x,i)$，权值 $w_k$，观测序列 $x=(x_1,x_2,...,x_n)$

输出：最优路径 y

步骤：

1. 初始化：

$$
\delta_1(j) = \sum_{k=1}^Kw_kf_k(y_o=start,y_1=j,x,i) , j=1,2,...,m
$$

2. 递归：

$$
\delta_i(l) = \max\{ \delta_{i-1}(j)+\sum_{k=1}^Kw_kf_k(y_{i-1}=j,y_i=l,x,i))\}
$$

$$
\Psi_i(l) = \arg \max_{j} \{ \delta_{i-1}(j)+\sum_{k=1}^Kw_kf_k(y_{i-1}=j,y_i=l,x,i))\}
$$

2. 回溯：

$$
y_n^* = \arg \max_j \delta_n(j)
$$

$$
y_{i}^* = \Psi_{i+1}(y_{i+1}^*)
$$

# 3. CRF应用

## 3.1 中文分词

分词的输入为一句话（文字序列）x，要预测每个词的标签 {B,E,M,S} ，对应到CRF。

通过训练数据语料，可以学习参数wk；状态特征根据语料构造，转移特征固定为4x4=16个。

再预测测试数据的文字序列，即CRF的维特比算法。

## 3.2 词性标注

与中文分词类似。