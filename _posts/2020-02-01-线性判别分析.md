---
layout:     post
title:      统计学习方法---LDA
subtitle:   Linear Discriminant Analysis，LDA
date:       2020-02-01
author:     weber
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - machine learning
---
Linear Discriminant Analysis，LDA

# 1. LDA简介
线性判别分析（Linear Discriminant Analysis，LDA）也称为 Fisher 判别分析，是一种监督学习的降维技术。

其核心思想为：**投影后的类内方差最小，类间方差最大。**	![img](https://tva1.sinaimg.cn/large/00831rSTgy1gd64n64rfkj30qr0acjs3.jpg)

假设我们有两类数据分别为红色和蓝色，如图所示，这些数据特征是二维的，我们希望将这些数据投影到一维的一条直线，让每一种类别数据的投影点尽可能的接近，而红色和蓝色数据中心之间的距离尽可能的大。

# 2. LDA二分类推导

数据集：$D=\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\}$，其中样本$x_i$为n维向量，$y_i \in \{0,1\}$。 

定义：$N_j$ 为第 j 类样本的个数（二分类只有0,1两类），$X_j$为第 j 类样本的集合

第 j 类的均值：
$$
\mu_j = \frac{1}{N_j} \sum_{x_i \in X_j} x_i
$$
第 j 类的协方差（严格来说缺少分母部分）：
$$
\Sigma_j = \sum_{x_i \in X_j}(x_i - \mu_j)(x_i-\mu_j)^T
$$
我们将两类数据投影到一条直线上，假设投影直线为向量 $w$，则任一个样本投影后向量为$w^Tx_i$，两类的中心点投影为$w^Tu_j$，投影点的协方差为$w^T\Sigma_j w$。

则我们的优化目标为：
$$
\arg \max J(w) = \frac{||w^T\mu_0-w^T\mu_1||_2^2}{w^T\Sigma_0w + w^T \Sigma_1w} = \frac{w^T(\mu_0-\mu_1)(\mu_0-\mu_1)^Tw}{w^T(\Sigma_0+\Sigma_1)w}
$$
我们一般定义类内散度矩阵为：
$$
S_w = \Sigma_0 + \Sigma_1
$$
类间散度矩阵为：
$$
S_b =(\mu_0-\mu_1)(\mu_0-\mu_1)^T
$$
则优化目标可写为：
$$
\arg \max J(w) = \frac{w^T S_b w}{w^TS_ww}
$$
该形式符合广义瑞利商的形式。可以由瑞利商的性质知道，$J(w')$的最大值为$S_w^{-\frac{1}{2}}S_bS_w^{-\frac{1}{2}}$的最大特征值，$w'$ 为对应的特征向量。而$S_b^{-1}S_w$与$S_b^{-\frac{1}{2}}S_wS_b^{-\frac{1}{2}}$的特征值相同，$S_w^{-1}S_b$与$S_w^{-\frac{1}{2}}S_bS_w^{-\frac{1}{2}}$的特征向量满足$w=S_w^{-\frac{1}{2}}w'$。

对于二类的时候，$𝑆_𝑏𝑤$的方向恒平行于$(\mu_0−\mu_1)$：
$$
S_bw=\lambda(\mu_0-\mu_1)
$$
将其带入：
$$
(S_w^{-1}S_b) w= \lambda w
$$
可以得到：
$$
w = S_w^{-1}(\mu_0-\mu_1)
$$
可见，我们确定了原始二类样本的均值和方差，就可以确定最佳的投影方向$w$了。

# 3. LDA多分类推导

由二分类类比到多分类，此时投影不再是一条直线，而是一个平面，假设投影到的低维空间的维度为 d ，对应的基向量为 $(w_1,w_2,...,w_d)$，基向量组成矩阵$W \in \mathbb{R}^{n \times d}$。

则优化目标为：
$$
\arg\max J(W) = \frac{W^T S_bW}{W^TS_wW}
$$
其中，$S_b= \sum_{j=1}^k N_j(\mu_j-\mu)(\mu_j-\mu)^T$，$S_w=\sum_{j=1}^k \sum_{x_i \in X_j}(x_i-\mu_j)(x_i-\mu_j)^T$

由于分母上下都是矩阵而非标量，无法进行优化，因此使用替代函数：
$$
\arg\max J(W) = \frac{\prod_{diag}W^T S_bW}{\prod_{diag}W^TS_wW}
$$
其中，$\prod_{diag}A$为矩阵A主对角线元素的乘积。接下来与二分类类似。

# 4. LDA降维流程总结

输入：数据集$D=\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\}$，其中样本$x_i$为n维向量，$y_i \in \{C_1,C_2,...,C_k\}$，降维到的维度d。

输出：降维后的数据集 $D'$。

1. 计算类内散度矩阵$S_w$。
2. 计算类间散度矩阵$S_b$。
3. 计算矩阵$S_w^{-1}S_b$。
4. 计算矩阵$S_w^{-1}S_b$的最大的 d 个特征值和对应的 d 个特征向量，得到投影矩阵 W。
5. 对每个样本进行转换：$z_i = W^Tx_i$，得到输出样本集$D'=\{(z_1,y_1),(z_2,y_2),...,(z_n,y_n)\}$。



# 5. LDA优缺点

优点：

1. 降维过程中使用了类别这样的先验知识。
2. LDA在样本分类信息依赖均值而非方差的时候，比PCA更优。

缺点：

1. LDA不适合对非高斯分布的样本进行降维。
2. LDA降维最多降到类别数k-1的维数。
3. LDA在样本分类信息依赖方差而不是均值的时候，降维效果不好。

# 6. LDA与PCA的区别

LDA用于降维，与PCA有很多相同点和不同点。

相同点：

1. 假设数据符合高斯分布。
2. 降维都使用了矩阵分解的思想。

不同点：

1. LDA有监督，PCA无监督。
2. LDA最多降维到类别数k-1的维度，PCA无限制。（因为投影矩阵W是利用类别数得到的）
3. LDA除了降维，还可以用于分类。
4. LDA选择分类性能最好的投影方向，PCA选择样本点投影具有最大方差的方向。

# 7. LDA与LR的区别

LDA也可以用于分类：当一个新的样本到来后，我们可以将它投影，然后将投影后的样本特征分别带入各个类别的高斯分布概率密度函数，计算它属于这个类别的概率，最大的概率对应的类别即为预测类别。

LDA与LR的区别：

1. 假设不同：LDA假设数据服从正态分布，LR无此假设；
2. 参数估计方式不同：LDA计算均值和协方差，然后带入判别式；LR使用极大似然估计参数；
3. 模型意义不同：LDA属于生成模型，用到样本的先验信息，极大化后验概率；LR属于判别模型，只考虑给丁x时的条件概率。



# 参考资料

 [刘建平---线性判别分析LDA原理总结](https://www.cnblogs.com/pinard/p/6244265.html)