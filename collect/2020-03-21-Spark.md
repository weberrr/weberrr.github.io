最近在学习Spark，内容比较多，写些笔记进行整理。

因为python，java都比较熟悉，故使用scala来写本文，算是学习scala。

# 2. RDD编程

弹性分布式数据集（Resilient Distributed Dataset，RDD）是Spark对数据的核心抽象。Spark会自动将RDD中的数据分发到集群上，并将操作并行化执行。

## 2.1 创建RDD

创建RDD有两种方式：

- 读取外部数据集
- 在驱动程序中对一个集合进行并行化

### 2.1.1 读取外部数据集

Spark支持本文文件，也支持任何其他Hadoop的输入格式（如HDFS，HBase）。

```scala
//sc为Spark Shell中默认创建的SparkContext对象，用户告诉Spark如何访问集群
val distData = sc.textFile("data.txt")
```

小tips：

1. 如果是本地文件路径，需要将文件复制到所有工作服务器；
2. 如果是多个文件，可以使用 `sc.wholeTextFiles`读取，以（文件名，内容）对返回；
3. 对于其他Hadoop的inputFormat，可以使用`sc.hadoopRDD`的读取。

### 2.1.2 并行化集合

可以将程序中一个已有的集合，传给 `parallelize()`方法，来创建RDD。

```scala
val data = List(1,2,3,4,5)
val distData = sc.parallelize(data)
```

小tips：

除了开发和测试外，这种方式用的并不多，因为要把整个数据集先放入一台机器的内存中。

## 2.2 RDD操作

RDD支持两种类型的操作：转化（transformations）和 行动（actions）。

转化操作会返回一个新的RDD，行动操作会向驱动器程序返回结果或把结果写入外部系统。

### 2.2.1 转化操作

RDD的转化操作是 **惰性求值** 的，意味着当我们对RDD调用转化操作时，操作不会立即执行。这样可以把一些操作合并，来减少计算步骤。仅当行动操作要求将结果返回给驱动程序时，才计算转化。

常见的转化操作有：

对单个RDD的转化操作：map()，flatmap()，filter()，distinct()，sample()

对两个RDD的转化操作：union()，intersection()，subtract()，cartesian()、

以数据为{1,2,3,3}的`rdd1`和{3,4,5}的`rdd2`作为输入，示例如下：

|             函数名              |                             目的                             |           示例           |       结果       |
| :-----------------------------: | :----------------------------------------------------------: | :----------------------: | :--------------: |
|           map( func )           |        将函数应用于RDD中的每个元素，返回值构成新的RDD        |    `rdd1.map(x=>x+1)`    |    {2,3,4,4}     |
|         flatmap( func )         |       将函数应用于RDD的每个元素，返回值拍平，构成新RDD       |   `rdd1.map(x.to(3))`    | {1,2,3,2,3,3,3}  |
|         filter( func )          |               返回满足 func 的元素构成的新RDD                |  `rdd1.filter(x=>x!=1)`  |     {2,3,3}      |
|           distinct()            |                       去重，构成新RDD                        |    `rdd1.distinct()`     |     {1,2,3}      |
| sample(withReplacement, [seed]) | 对RDD采样，参数1控制是否可以重复取元素，参数2控制每个元素被抽取的概率 | `rdd1.sample(false,0.1)` |      不确定      |
|             union()             |                            取并集                            |    `rdd1.union(rdd2)`    | {1,2,3,3,3,4,5} |
| intersection() | 取交集 | `rdd1.intersection(rdd2)` | {3} |
| substract() | 取补集 | `rdd1.substract(rdd2)` | {1,2} |
| cartesian() | 两个RDD的笛卡尔积 | `rdd1.cartesian(rdd2)` | {(1,3),(1,4),...(3,5)} |

如果我们希望多次使用一个RDD，就不能再让其惰性求值了，需要将其持久化：

```scala
rdd.persist(StorageLevel.DISK_ONLY)
```

### 2.2.2 行动操作

常见的行动操作如下表所示。以数据为{1,2,3,3}的`rdd1`作为输入，则：

|                 函数名                 |                  目的                  |                             示例                             |        结果         |
| :------------------------------------: | :------------------------------------: | :----------------------------------------------------------: | :-----------------: |
|               collect()                |          返回RDD中的所有元素           |                       `rdd1.collect()`                       |      {1,2,3,3}      |
|                count()                 |            RDD中元素的个数             |                        `rdd1.count()`                        |          4          |
|             countByValue()             |        各元素在RDD中出现的次数         |                    `rdd1.countByValue()`                     | {(1,1),(2,1),(3,2)} |
|               take(num)                |         从RDD中返回 num 个元素         |                        `rdd1.take(2)`                        |        {1,2}        |
|                top(num)                |    从RDD中返回最大(小)的 num个元素     |                        `rdd1.take(2)`                        |        {3,3}        |
| takeSample(withReplacement,num,[seed]) |        从RDD中返回任意一些元素         |                  `rdd1.takeSample(false,1)`                  |       不确定        |
|              reduce(func)              |        并行整合RDD中的所有元素         |                  `rdd1.reduce((x,y)=>x+y)`                   |          9          |
|            fold(zero)(func)            |    和reduce()相似，但需要提供初始值    |                  `rdd1.fold(0)((x,u)=>x+y)`                  |          9          |
|     aggregate(value)(seqOp,combOp)     | 和reduce()相似，但通常返回不同类型函数 | `rdd1.aggregate((0,0))((x,y)=>x._1+y,x._2+1),(x,y)=>x._1+y._1,x._2+y._2)` |        (9,4)        |
|foreach(func)|对RDD中每个元素使用函数|rdd.foreach(func)|无|

## 2.3 健值对RDD

健值对RDD以健值对的形式存储数据。

pairRDD的常用转化操作有：reduceByKey(), groupByKey(), keys(), values(), join(rdd)等。

pairRDD的常用的行动操作有：countBykey(), collectAsMap(), lookup(key)

# 3. 数据读入与保存

常见的数据源：

- 文件（文本，JSON，SequenceFile）
- Spark SQL中的结构化数据
- 数据库和第三方库

## 3.1 文件格式

读取/保存文本文件：

```scala
val input = sc.textFile("path")
input.saveAsTextFile("path")
```

读取/保存JSON：

读取时以文本文件读取，然后对JSON数据进行解析

```scala
val input = sc.textFile("file.json")
val result = input.flatMap(record=>{
  try{
    Some(map.readValue(record,classOf[Person]))
  }catch{
    case e:Exception => None
  }
})
```

读取/保存CSV：

```scala
val input = sc.textFile("file.csv")
val result = input.map(line=>
	val reader = new CSVReader(new StringReader(line));
	reader.readNext();
)
```

## 3.2 Spark SQL

把一条SQL给Spark SQL，让它对一个数据源进行查询（选出一些字段）


```scala
val hiveCtx = new HiveContext(sc)
val rows = hiveCtx.sql("SELECT name, age FROM users")
val firstRow = rows.first()
```

## 3.3 数据库

通过数据库提供的Hadoop连接器或者自定义的Spark连接器，Spark可以访问一些数据库系统。

```scala
def createConnection() = {
	Class.forName("com.mysql.jdbc,Driver").newInstance();
	DriverManager.gerConnection("jdbc:mysql://localhost/test?");
}

def extractValues(r: ResultSet) = {
  (r.getInt(1),r.getString(2))
}

val data = new JdbcRDD(sc,
                       createConnection, "SELECT * FROM panda WHERE ? <= id AND id <= ?",
                       lowerBound = 1, upperBound = 3,newPartitions=2,mapRow= extractValues)
```

